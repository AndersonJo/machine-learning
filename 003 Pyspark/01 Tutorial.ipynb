{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts: 8\n",
      "CPU times: user 69 µs, sys: 57 µs, total: 126 µs\n",
      "Wall time: 132 µs\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "counts = words.count()\n",
    "\n",
    "%time print(f'counts: {counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scala',\n",
       " 'java',\n",
       " 'hadoop',\n",
       " 'spark',\n",
       " 'akka',\n",
       " 'spark vs hadoop',\n",
       " 'pyspark',\n",
       " 'pyspark and spark']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ForEach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    print(x)\n",
    "    \n",
    "words.foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pyspark', 'pyspark and spark']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.filter(lambda x: 'py' in x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scala', 1, 3),\n",
       " ('java', 1, 3),\n",
       " ('hadoop', 1, 3),\n",
       " ('spark', 1, 3),\n",
       " ('akka', 1, 3),\n",
       " ('spark vs hadoop', 1, 3),\n",
       " ('pyspark', 1, 3),\n",
       " ('pyspark and spark', 1, 3)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda x: (x, 1, 3)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "nums = sc.parallelize([1, 2, 3, 4, 5, 10])\n",
    "nums.reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join\n",
    "\n",
    "1. **join**: 두개의 RDD에 모두 존재하는 elements만 join이 되고, 나머지는 제외\n",
    "2. **fullOuterJoin**: 모든 elements를 join 시킨다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ml', (10, 5)), ('spark', (1, 2))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([('spark', 1), ('ml', 10), ('power', 2)])\n",
    "y = sc.parallelize([('spark', 2), ('ml', 5), ('happy', 3)])\n",
    "joined = x.join(y)\n",
    "joined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ml', (10, 5)),\n",
       " ('power', (2, None)),\n",
       " ('spark', (1, 2)),\n",
       " ('happy', (None, 3))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.fullOuterJoin(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ml', (10, 5)), ('power', (2, None)), ('spark', (1, 2))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.leftOuterJoin(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache\n",
    "\n",
    "\"MEMORY_ONLY\" 일경우.. 메모리에 RDD를 persist시킨다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.cache()\n",
    "words.persist().is_cached"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
